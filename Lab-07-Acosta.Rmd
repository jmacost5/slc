---title: "Week 7 - Final Project"
author: "Jestrii Acosta"
date: '2022-05-03'
output: html_document
---
```{r}
library( geojsonio )   # read shapefiles
library( sp )          # work with shapefiles
library( sf )          # work with shapefiles - simple features format
library( mclust )      # cluster analysis 
library( tmap )        # theme maps
library( ggplot2 )     # graphing 
library( ggthemes )    # nice formats for ggplots
library( dplyr )       # data wrangling 
library( pander )      # formatting RMD tables
library( tidycensus )

library( cartogram )  # spatial maps w/ tract size bias reduction
library( maptools )   # spatial object manipulation 
library(stringr)
library(fun)

```



```{r, include=FALSE}
crosswalk <- read.csv( "https://raw.githubusercontent.com/DS4PS/cpp-529-master/master/data/cbsatocountycrosswalk.csv",  stringsAsFactors=F, colClasses="character" )

# search for citie names by strings, use the ^ anchor for "begins with" 


```



```{r}
grep( "^SALT LAKE", crosswalk$msaname, value=TRUE )  
```



```{r}
these.slc <- crosswalk$msaname == "SALT LAKE CITY-OGDEN, UT"
these.fips <- crosswalk$fipscounty[ these.slc ]
these.fips <- na.omit( these.fips )
head( these.fips ) %>% pander()
```



```{r}
state.fips <- substr( these.fips, 1, 2 )
county.fips <- substr( these.fips, 3, 5 )

cbind( these.fips, state.fips, county.fips ) %>% pander()
```



```{r}
slc.pop <-
get_acs( geography = "tract", variables = "B01003_001",
         state = "49", county = county.fips[state.fips=="49"], geometry = TRUE ) %>% 
         select( GEOID, estimate ) %>%
         dplyr::rename( POP=estimate )
```



```{r}
salt_lake <- slc.pop

# make sure there are no empty polygons
salt_lake <- salt_lake[ ! st_is_empty( salt_lake ) , ]

# convert sf map object to an sp version
salt_lake.sp <- as_Spatial( salt_lake )

class( salt_lake.sp )
```



```{r, include=FALSE}
plot( salt_lake.sp )
```



```{r}
# project map and remove empty tracts
salt_lake.sp <- spTransform( salt_lake.sp, CRS("+init=epsg:3395"))
salt_lake.sp <- salt_lake.sp[ salt_lake.sp$POP != 0 & (! is.na( salt_lake.sp$POP )) , ]

# convert census tract polygons to dorling cartogram
# no idea why k=0.03 works, but it does - default is k=5
salt_lake.sp$pop.w <- salt_lake.sp$POP / 9000 # max(msp.sp$POP)   # standardizes it to max of 1.5
salt_lake_dorling <- cartogram_dorling( x=salt_lake.sp, weight="pop.w", k=0.05 )
plot( salt_lake_dorling )
```



```{r}
class(salt_lake_dorling)
```


```{r}
# user-defined bounding box to move slocer to subjects 
bb <- st_bbox( c( xmin =  -13647722, xmax = -13567392, 
                  ymax = 6084955, ymin = 5941032 ), 
               crs = st_crs("+init=epsg:3395"))

tm_shape( salt_lake_dorling, bbox=bb ) + 
  tm_polygons( col="POP", n=10, style="quantile", palette="Spectral" ) +
  tm_layout( "Salt Lake City", title.position=c("right","top") )

```

```{r}
keep.these <- c("pnhwht12", "pnhblk12", "phisp12", "pntv12", "pfb12", "polang12", 
"phs12", "pcol12", "punemp12", "pflabf12", "pprof12", "pmanuf12", 
"pvet12", "psemp12", "hinc12", "incpc12", "ppov12", "pown12", 
"pvac12", "pmulti12", "mrent12", "mhmval12", "p30old12", "p10yrs12", 
"p18und12", "p60up12", "p75up12", "pmar12", "pwds12", "pfhh12")

dat1 <- salt_lake_dorling@data

URL <- "https://github.com/DS4PS/cpp-529-master/raw/master/data/ltdb_std_2010_sample.rds"
census.dat <- readRDS(gzcon(url( URL )))
# can merge an sf object and data.frame
dat1 <- merge( dat1, census.dat, by.x="GEOID", by.y="tractid" )

dat2 <- select( dat1, keep.these )
dat3 <- apply( dat2, 2, scale )
head( dat3[,1:6] ) %>% pander()
```


```{r}
set.seed( 1234 )
fit <- Mclust( dat3 )
salt_lake_dorling$cluster <- as.character( fit$classification )
summary( fit )
```


# Add Census Data


```{r}
URL1 <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-2000.rds"
d1 <- readRDS( gzcon( url( URL1 ) ) )

URL2 <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-2010.rds"
d2 <- readRDS( gzcon( url( URL2 ) ) )

URLmd <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-META-DATA.rds"
md <- readRDS( gzcon( url( URLmd ) ) )

d1 <- select( d1, - year )
d2 <- select( d2, - year )

d <- merge( d1, d2, by="tractid" )
d <- merge( d, md, by="tractid" )
head(d)
```


```{r}
library(fun)
# regexp1 <- sub('^[^-]*-([0-9]+).*?','\\1',d$tractid)
# d$geoid1 <- str_extract(d$tractid, regexp1)
# step1 <- substr(d$geoid1, start = 1, stop = 2)
# step2 <- substr(d$geoid1, start = 4, stop = 6)
# step3 <- substr(d$geoid1, start = 8, stop = 13)
# d$geoid <- paste0(step1,step2,step3)
# d$geoid <- as.numeric(as.character(d$geoid))


# STANDARDIZE GEO IDs

# note the current geoid format for the LTDB census data: 
# FIPS-STATE-COUNTY-TRACT:  fips-01-001-020100  

x <- d$tractid 
# head( x )
# [1] "fips-01-001-020100" "fips-01-001-020200" "fips-01-001-020300"
# [4] "fips-01-001-020400" "fips-01-001-020500" "fips-01-001-020600"

# remove non-numeric strings 
x <- gsub( "fips", "", x )
x <- gsub( "-", "", x )
# head( x )
# [1] "01001020100" "01001020200" "01001020300" "01001020400" "01001020500"
# [6] "01001020600"

# drop leading zeros 
x <- as.numeric( x )

# remember to add the variable back to the census dataset
d$tractid2 <- x 

```

```{r}
d <- select( d, tractid, 
             mhmval00, mhmval12, 
             hinc00, 
             hu00, vac00, own00, rent00, h30old00,
             empclf00, clf00, unemp00, prof00,  
             dpov00, npov00,
             ag25up00, hs00, col00, 
             pop00.x, nhwht00, nhblk00, hisp00, asian00,
             cbsa, cbsaname, pnhwht12, 
             pnhblk12, phisp12, pntv12, pfb12, polang12, 
             phs12, pcol12, punemp12, pflabf12, pprof12, pmanuf12, 
             pvet12, psemp12, hinc12, incpc12, ppov12, pown12, 
             pvac12, pmulti12, mrent12, p30old12, p10yrs12, 
             p18und12, p60up12, p75up12, pmar12, pwds12, pfhh12)


 
d <- 
  d %>%
  mutate( # percent white in 2000
          p.white = 100 * nhwht00 / pop00.x,
          # percent black in 2000
          p.black = 100 * nhblk00 / pop00.x,
          # percent hispanic in 2000
          p.hisp = 100 * hisp00 / pop00.x, 
          # percent asian in 2000
          p.asian = 100 * asian00 / pop00.x,
          # percent high school grads by age 25 in 2000 
          p.hs = 100 * (hs00+col00) / ag25up00,
          # percent pop with college degree in 2000
          p.col = 100 * col00 / ag25up00,
          # percent employed in professional fields in 2000
          p.prof = 100 * prof00 / empclf00,
          # percent unemployment  in 2000
          p.unemp = 100 * unemp00 / clf00,
          # percent of housing lots in tract that are vacant in 2000
          p.vacant = 100 * vac00 / hu00,
          # percent of housing owned in 2000
          p.owned = 100* own00 / hu00,
          # dollar change in median home value 2000 to 2010 
          pov.rate = 100 * npov00 / dpov00 )


# adjust 2000 home values for inflation 
mhv.00 <- d$mhmval00 * 1.28855  
mhv.10 <- d$mhmval12

# change in MHV in dollars
mhv.change <- mhv.10 - mhv.00


# drop low 2000 median home values
# to avoid unrealistic growth rates.
#
# tracts with homes that cost less than
# $1,000 are outliers
#mhv.00[ mhv.00 < 1000 ] <- NA

# change in MHV in percent
mhv.growth <- 100 * ( mhv.change / mhv.00 )

##Omit cases with growth rates above 200%
#mhv.growth[ mhv.growth > 200 ] <- NA


d$mhv.00 <- mhv.00
d$mhv.10 <- mhv.10
d$mhv.change <- mhv.change
d$mhv.growth <- mhv.growth

```




```{r}
library(fun)
# regexp1 <- sub('^[^-]*-([0-9]+).*?','\\1',d$tractid)
# d$geoid1 <- str_extract(d$tractid, regexp1)
# step1 <- substr(d$geoid1, start = 1, stop = 2)
# step2 <- substr(d$geoid1, start = 4, stop = 6)
# step3 <- substr(d$geoid1, start = 8, stop = 13)
# d$geoid <- paste0(step1,step2,step3)
# d$geoid <- as.numeric(as.character(d$geoid))


# STANDARDIZE GEO IDs

# note the current geoid format for the LTDB census data: 
# FIPS-STATE-COUNTY-TRACT:  fips-01-001-020100  

x <- d$tractid 
# head( x )
# [1] "fips-01-001-020100" "fips-01-001-020200" "fips-01-001-020300"
# [4] "fips-01-001-020400" "fips-01-001-020500" "fips-01-001-020600"

# remove non-numeric strings 
x <- gsub( "fips", "", x )
x <- gsub( "-", "", x )
# head( x )
# [1] "01001020100" "01001020200" "01001020300" "01001020400" "01001020500"
# [6] "01001020600"

# drop leading zeros 
x <- as.numeric( x )

# remember to add the variable back to the census dataset
d$tractid2 <- x 

slc <- merge( salt_lake_dorling, d, by.x="GEOID", by.y="tractid2")


head( slc@data ) %>% pander()
```


```{r}
names( slc ) %>% sort()
```



```{r}
row.ids <- sapply( slot( slc, "polygons" ), function(x) slot( x, "ID" ) )
row.names( slc ) <- row.ids
```

```{r}
slc <- spTransform( slc, CRS("+proj=longlat +datum=WGS84") )
geojson_write( slc, file="pitt_dorling_10.geojson", geometry="polygon" )
```



```{r}
row.ids <- sapply( slot( slc, "polygons" ), function(x) slot( x, "ID" ) )
row.names( slc ) <- row.ids
```

```{r}
slc <- spTransform( slc, CRS("+proj=longlat +datum=WGS84") )
geojson_write( slc, file="slc_dorling.geojson", geometry="polygon" )
```

